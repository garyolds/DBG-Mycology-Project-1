{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 1 Tutorial\n",
    "last modified 28 December 2021<br>\n",
    "Code by Gary Olds and Jessie Berta-Thompson<br>\n",
    "Instructions by Gary Olds and Andrew Wilson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">Quick Referece Guide to Keyboard Commands\n",
    "Shift-Return = Run code in cell<br>\n",
    "Option-Return = Add cell below<br>\n",
    "    \n",
    "**Esc = Enter Command Mode**<br>\n",
    "While in command mode\n",
    "    B = Add cell below<br>\n",
    "    A = Add cell above<br>\n",
    "    DD = Delete Cell<br>\n",
    "    X = Cut Cell <br>\n",
    "    M = **Markdown** mode<br>\n",
    "    E = Delete Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip and relocate your .fastq.gz files\n",
    "Once you recive your .fastq.gz files from the GBRC, you will need to unzip them and then organize the files and data so you can analyze the sequences. Make a working directory that you plan to do all of your analysis in. Place your unzipped .fastq files in this directory.\n",
    "\n",
    "Prior to analyzing anything you need to perform some simple commands for setting up your workspace. This includes setting the filepath for working on sequences, loading the libraries with the commands for analying your sequences, and defining some original functions to run the pipeline\n",
    "\n",
    "### Setup\n",
    "Start setting up your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAINdir = \"/Users/andrew.wilson/Russula_BC\"\n",
    "TRIMMEDdir = f\"{MAINdir}/NoPrimers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {MAINdir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##    BASICS\n",
    " \n",
    "import os                #command-line like functions, for operating system interface (finding files)\n",
    "import subprocess        #recommended way of running command line programs from within python\n",
    "import numpy as np       #for math and arrays\n",
    "import shutil            #used to move files around\n",
    "import sys               #helps with reading and writing onto text files\n",
    "from tqdm import tqdm    #a progress bar for for-loops (lets you see progress in actively running loops)\n",
    "from time import time    #use to track time and measure how long chunks are taking to run\n",
    "import shutil            #for moving files from directory to directory\n",
    "import csv               #writing and reading csv\n",
    " \n",
    "##    FOR DNA\n",
    " \n",
    "from Bio import Seq                 #reading in and manipulating sequence data\n",
    "from Bio import SeqIO               #for reading and writing fasta/qs\n",
    "from Bio.SeqRecord import SeqRecord #creating sequence records that are objects and not just strings\n",
    "from Bio.SeqUtils import GC         #for calculating GC content\n",
    " \n",
    "##    FOR FIGURES\n",
    " \n",
    "import matplotlib.pyplot as plt          #basic plotting tools that let you do most of what you'll need to do, set up as a shortcut\n",
    "import matplotlib                        #get ALL of matplotlib for fancier tools\n",
    "                                         #add specific other matplotlib imports as needed\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42 #make saved vector pdf files use fonts instead of lines/shapes for text\n",
    "                                         #(essential weird line for saving vector pdfs for downstream editing in illustrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions: Allows for downstream evaluation of sequence files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table(alist):\n",
    "    \"\"\"\n",
    "    Description: Define a function that prints and stores a frequency table for \n",
    "    a list's contents. This is useful for quick summarizing and quality-control steps. \n",
    "    Input:   alist\n",
    "    Output: (1) printed frequency table for how many instances of each value are in list\n",
    "            (2) pair of lists, unique items list and counts list, order-matching. \n",
    "    \"\"\"\n",
    "    alist = list(alist) #make sure a list\n",
    "    uniqueitems = sorted(list(set(list(alist)))) # get unique entries, make a list again, sort list\n",
    "    counts = []#place to store frequency counts, one count for each unique item in input list\n",
    "    print(\"value\\tinstances\") #print header for table\n",
    "    for item in uniqueitems:\n",
    "        counts.append(alist.count(item))\n",
    "    for i in range(len(uniqueitems)):\n",
    "        print(str(uniqueitems[i])+\"\\t\"+str(counts[i]))\n",
    "    return(uniqueitems, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the table function written above is working. Shows frequency in the given list\n",
    "TestList=[1,1,1,2,2,2,3,4,5,5]\n",
    "table(TestList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savedict(indict, outfile):\n",
    "    \"\"\"\n",
    "    Define a function to write a data dictionary out to tab file\n",
    "    Save a dictionary of form header_key:data_list\n",
    "    to tab-delimited file, given dictionary and file name string.\n",
    "    Creates a table with columns representing keys and rows items in value lists.\n",
    "    \"\"\"\n",
    "    #Get keys into a stable list\n",
    "    ordered_keys = list(indict.keys())\n",
    "\n",
    "    #Initialize list of lines to save with header line based on keys\n",
    "    filerows = [\"\\t\".join(ordered_keys)]\n",
    "\n",
    "    #Example key (use first one)\n",
    "    examplekey = ordered_keys[0]\n",
    "\n",
    "    #Loop over rows in dataset\n",
    "    for i in range(len(indict[examplekey])):\n",
    "        #place to store entries for each column for this row as a list\n",
    "        thisrow = []\n",
    "        #Loop over columns\n",
    "        for key in ordered_keys:\n",
    "            thisrow.append(str(indict[key][i]))#for join method, has to be a string.\n",
    "\n",
    "        #Join items in a file line\n",
    "        filerows.append(\"\\t\".join(thisrow))\n",
    "\n",
    "    #Save lines to file\n",
    "    with open(outfile, 'w') as out:\n",
    "        for line in filerows:\n",
    "            out.write(line+\"\\n\")\n",
    "    # Report back\n",
    "    print(f\"Saving dictionary to file {outfile}, {(len(filerows)-1)} lines of data, {len(indict.keys())} columns.\")\n",
    "    # Return nothing\n",
    "    return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_fastqs(files, outfilename):\n",
    "    \"\"\"\n",
    "    Define a function for getting fastq file summaries\n",
    "    Warning, it does math with quality scores so it's slow.\n",
    "    input a list of files (full or relative path)\n",
    "    output a tab delimited file of summary stats \n",
    "    about the sequences in the files\n",
    "    \"\"\"\n",
    "    #Initialize summary stat table, one line per fastq file\n",
    "    perfile = {}\n",
    "    #Looking at the whole file\n",
    "    perfile['path'] = []\n",
    "    perfile['file'] = []\n",
    "    perfile['n sequences'] = []\n",
    "    perfile['total bases'] = []\n",
    "    #Looking at the lengths of the sequences\n",
    "    perfile['mean sequence length'] = []\n",
    "    perfile['standard deviation sequence length'] = []\n",
    "    perfile['median sequence length'] = []\n",
    "    perfile['minimum sequence length'] = []\n",
    "    perfile['maximum sequence length'] = []\n",
    "    #Looking at the pool of individual base quality scores\n",
    "    perfile['mean total base quality score'] = []\n",
    "    perfile['standard deviation total base quality score'] = []\n",
    "    perfile['min total base quality score'] = []\n",
    "    perfile['max total base quality score'] = []\n",
    "    #Looking at the read average quality scores\n",
    "    perfile['mean read average quality score'] = []\n",
    "    perfile['standard deviation read average quality score'] = []\n",
    "    perfile['min read average quality score'] = []\n",
    "    perfile['max read average quality score'] = []\n",
    "    \n",
    "    #Loop through files to collect information\n",
    "    for file in tqdm(files):\n",
    "        lengths = [] #read lengths\n",
    "        qualities = []#qual values for all bases\n",
    "        mean_qualities = []#read mean qual averages\n",
    "        for rec in SeqIO.parse(file, 'fastq'):\n",
    "            lengths.append(len(str(rec.seq)))\n",
    "            seqqualities = rec.letter_annotations[\"phred_quality\"] \n",
    "            qualities.extend(seqqualities)\n",
    "            mean_qualities.append(np.mean(seqqualities))\n",
    "            \n",
    "        #Calculate and store summary stats. \n",
    "        perfile['path'].append(file)\n",
    "        perfile['file'].append(file.split(\"/\")[-1])\n",
    "        perfile['n sequences'].append(len(lengths))\n",
    "        perfile['total bases'].append(np.sum(lengths))\n",
    "        \n",
    "        #Looking at the lengths of the sequences\n",
    "        perfile['mean sequence length'].append(np.mean(lengths))\n",
    "        perfile['standard deviation sequence length'].append(np.std(lengths))\n",
    "        perfile['median sequence length'].append(np.median(lengths))\n",
    "        perfile['minimum sequence length'].append(np.min(lengths))\n",
    "        perfile['maximum sequence length'].append(np.max(lengths))\n",
    "        \n",
    "        #Looking at the pool of individual base quality scores\n",
    "        perfile['mean total base quality score'].append(np.mean(qualities))\n",
    "        perfile['standard deviation total base quality score'].append(np.std(qualities))\n",
    "        perfile['min total base quality score'].append(np.min(qualities))\n",
    "        perfile['max total base quality score'].append(np.max(qualities))\n",
    "        \n",
    "        #Looking at the read average quality scores\n",
    "        perfile['mean read average quality score'].append(np.mean(mean_qualities))\n",
    "        perfile['standard deviation read average quality score'].append(np.std(mean_qualities))\n",
    "        perfile['min read average quality score'].append(np.min(mean_qualities))\n",
    "        perfile['max read average quality score'].append(np.max(mean_qualities))\n",
    "        \n",
    "    savedict(perfile,outfilename)\n",
    "    return(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Evaluation: Checks to make sure all forward and reverse read data match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locate raw input files (includes already split replicates)\n",
    "allrawdir = os.listdir(MAINdir)\n",
    "forward_fastqs = []\n",
    "reverse_fastqs = []\n",
    "for file in allrawdir:\n",
    "    if file.endswith(\"_R1.fastq\"):\n",
    "        forward_fastqs.append(file)\n",
    "    elif file.endswith(\"_R2.fastq\"):\n",
    "        reverse_fastqs.append(file)\n",
    "\n",
    "#Sort to make sure pairs are in the same order in these lists\n",
    "forward_fastqs = sorted(forward_fastqs)\n",
    "reverse_fastqs = sorted(reverse_fastqs)\n",
    "\n",
    "#Report back on findings (and read to make sure sensible)\n",
    "print(f\"Found {len(forward_fastqs)} forward files with {len(set(forward_fastqs))} unique names, like {forward_fastqs[0]}\")\n",
    "print(f\"Found {len(reverse_fastqs)} reverse files with {len(set(reverse_fastqs))} unique names, like {reverse_fastqs[0]}\")\n",
    "\n",
    "#Check pair order on first few\n",
    "for i in range(1):\n",
    "    print(forward_fastqs[i].split(\"/\")[-1], reverse_fastqs[i].split(\"/\")[-1])    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Files: Evaluates all the reads in your files and gives summary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####     SUMMARY ONLY STEP\n",
    "\n",
    "summary_fastqs(forward_fastqs, \"1_Raw_forward_fastqs_summary.txt\")\n",
    "summary_fastqs(reverse_fastqs, \"1_Raw_reverse_fastqs_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Primers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward primer (ITS7f) - without spacer (on 5' end, distal to cuts), with R for degenerate base (cutadapt fine with that)\n",
    "forward_primer = Seq.Seq(\"GTGARTCATCGAATCTTTG\")            #converting to sequence object for handy complementing\n",
    "forward_primer_complement = forward_primer.reverse_complement()\n",
    "\n",
    "#Reverse primer (ITS4) - without spacer (on 5' end) and without 5 Ns for barcode region (between spacer and primer) \n",
    "reverse_primer = Seq.Seq(\"TCCTCCGCTTATTGATATGC\")\n",
    "reverse_primer_complement = reverse_primer.reverse_complement()\n",
    "\n",
    "#Make copies of those in simple string form for feeding to cutadapt\n",
    "fprimer = str(forward_primer)\n",
    "fprimer_rc = str(forward_primer_complement)\n",
    "rprimer = str(reverse_primer)\n",
    "rprimer_rc = str(reverse_primer_complement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run CutAdapt\n",
    "This will remove primers and spacers from the 3' end of the sequence reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save stdout to a file along the way - cutadapt prints lots of reporting information here\n",
    "with open(\"2_cutadapt_noprimers.txt\", \"w\") as stdouthandle:\n",
    "    \n",
    "    #Loop over pairs\n",
    "#    for ffile in tqdm(range(len(forward_fastqs))):\n",
    "    for ffile in tqdm(forward_fastqs):\n",
    "        \n",
    "        rfile = ffile.replace(\"_R1.fastq\",\"_R2.fastq\")\n",
    "        foutfastq = ffile.replace(\"R1.fastq\",\"R1_noprimer.fastq\")\n",
    "        routfastq = rfile.replace(\"R2.fastq\",\"R2_noprimer.fastq\")\n",
    "        \n",
    "#        #Compose command\n",
    "#        #-g, -G removes from 5' end for f and r reads\n",
    "#        cmd = f\"cutadapt -g {fprimer} -G {rprimer} -m 200 -o {foutfastq} -p {routfastq} {ffile} {rfile}\"\n",
    "\n",
    "        #Compose command\n",
    "        #-a, -A removes given primer from 3' end for f and r reads\n",
    "        cmd = f\"cutadapt -a {rprimer_rc} -A {fprimer_rc} -m 200 -o {foutfastq} -p {routfastq} {ffile} {rfile}\"\n",
    "\n",
    "#        print(cmd+\"\\n\")\n",
    "        \n",
    "        #Run command\n",
    "        subprocess.call(cmd, stdin=None, stdout=stdouthandle, stderr=subprocess.STDOUT, shell=True)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Primers Folder\n",
    "Make a folder to put your primer-free sequences in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a directory for files without primers & adapters, cutadapt results:\n",
    "%mkdir NoPrimers\n",
    "\n",
    "TOTAL = os.listdir()                  #take everything in the current directory and call it \"TOTAL\"\n",
    "MAIN = f\"{MAINdir}/\"                 #name the full directory path to the main directory \"MAIN\"\n",
    "NoPrimer = f\"{TRIMMEDdir}/\"          #name the destination directory \"NoPrimer\"\n",
    "for file in TOTAL:                    #for-loop regarding new variable \"file1\" in current directory\n",
    "    if (\"_noprimer.fastq\" in file):   #select files with \"_TPaired.fastq\" in their name (these were the files, forward and reverse, that did not lose their pair in trimming/filtering)\n",
    "        src = MAIN+file               #define the source location of the file in question\n",
    "        dst = NoPrimer+file           #define the destination location of the file in question\n",
    "        shutil.move(src,dst)          #move the file from source to destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run read files through DADA2.\n",
    "Move operations over to R Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
